{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced dimensionality reduction techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates advanced dimensionality reduction techniques such as Principal Component Analysis (PCA), Multi-Dimensional Scaling (MDS), and t-Distributed Stochastic Neighbor Embedding (t-SNE) on both image and text datasets. The project provides a hands-on exploration of how these techniques can be used to reduce high-dimensional data into lower-dimensional spaces while preserving meaningful patterns and structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "In this notebook we will:\n",
    "* Explain the following advanced dimensionality reduction techniques:\n",
    "    * Principal component analysis\n",
    "    * Multi-dimensional scaling\n",
    "    * t-SNE\n",
    "* Implement these techniques on an image dataset.\n",
    "* Implement these techniques on a text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction on images\n",
    "This notebook will provide a high-level overview of some of the more advanced techniques for dimensionality reduction. We will be using the [handwritten digits data set](http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits), which is a collection of images of handwritten digits between 0 and 9. The data was generated by a total of 43 people, who wrote a total of 5620 digits by hand which were then digitised and processed into 8x8 greyscale images.\n",
    "\n",
    "We'll start by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection, preprocessing)\n",
    "from matplotlib import offsetbox\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted',\n",
    "        rc={'figure.figsize': (15,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and inspecting the data\n",
    "The digits dataset is included as part of the `sklearn` library, which means loading it into the notebook is a breeze. For this train, we will simplify things by only looking at the first six digits in the dataset. We can use the `n_class` argument in the `load_digits()` function to select only the numbers from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits(n_class=6)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print(\"Total number of samples: \", n_samples)\n",
    "print(\"Features per sample: \", n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the image shows a selection from the 64-dimensional digits dataset. Each digit is represented as an 8x8 array of pixels, with values ranging between 0 (white) and 16 (black)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\n",
    "for idx, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)\n",
    "    ax.axis(\"off\")\n",
    "_ = fig.suptitle(\"A selection from the 64-dimensional digits dataset\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same concept applies to our dataset, only the arrays have been strung out into single rows of 64 values, ranging between 0 and 16. Let's take a look at the first digit in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you tell that the first array was a zero? Unless you are some sort of savant genius, you probably wouldn't be able to tell which digit is represented by looking only at the array values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction techniques\n",
    "Now what we are going to try and do is reduce the dimensionality of the entire dataset to just two dimensions (down from 64 dimensions â€“ one per pixel). We will use a few different dimensionality reduction techniques, and at each stage, we will plot the data and see how it is distributed in these two dimensions.   \n",
    "\n",
    "What is very important to note here is that none of these algorithms will be shown the labels of the data; they will be entirely unsupervised. In each case, we will plot the data in two dimensions, but include the known labels of the data points in the plots for our own validation.   \n",
    "\n",
    "For plotting, we are going to use the same `plot_embedding()` function defined in the original `sklearn` example code. This does a fantastic job of plotting the digits in a two-dimensional space, so there is no need to reinvent the wheel here.   \n",
    "\n",
    "**Note:** You will see the word **\"embedding\"** used in some of the comments and plots below. Simply put, an embedding is a representation of a vector in a different feature space. So in this case, the original digits exist as 64-dimensional arrays and are then reduced to just two dimensions. The resulting two-dimensional vectors are referred to as the embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and visualise the embedding vectors\n",
    "def plot_embedding(X, title=None):\n",
    "    \n",
    "    # normalise data\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  # just something big\n",
    "        for i in range(X.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-3:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_images = np.r_[shown_images, [X[i]]]\n",
    "            imagebox = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
    "                X[i])\n",
    "            ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA)\n",
    "The objective of PCA is to decompose a dataset into mutually orthogonal components that **each maximise the variance in the dataset**.   \n",
    "\n",
    "We will use PCA to decompose the dataset into the first two principal components, which will contain the largest and second-largest amounts of variance, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing PCA projection\")\n",
    "t0 = time()\n",
    "X_pca = decomposition.PCA(n_components=2).fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"Finished PCA projection in \" + str(t1-t0) + \"s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before using the awesome `plot_embedding()` function, let's plot the data, without labels, in the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1],\n",
    "                     sizes=(10, 200))\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you spot any obvious clusters? Perhaps one or two on the right side of the plot?   \n",
    "\n",
    "Now, let's plot the data with labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(X_pca,\n",
    "               \"Principal Components projection of the digits (time %.2fs)\" %\n",
    "               (t1 - t0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that digits do seem to group together, but there isn't the clearest separation between the different digits.   \n",
    "\n",
    "Now, let's take a look at some other techniques and see how they perform.   \n",
    "\n",
    "### Multi-dimensional scaling (MDS)\n",
    "The goal of MDS is to map features to a low-dimensional space **while preserving the distances** between observations in a given dataset.   \n",
    "\n",
    "MDS can be performed using algorithms that are either *metric* or *non-metric*. Non-metric approaches are typically used to preserve ordinality (order) within data. This is more of a necessity when there are categorical features present in the data. Since the data used here is entirely numeric, we will use a metric approach.   \n",
    "\n",
    "The **stress** is a measure of the degree to which distances between points in the original feature space correspond with the distances in the low-dimensional space. A lower stress value is preferred, and it is this quantity that is minimised by MDS.\n",
    "\n",
    "For more information on MDS, read the [`sklearn` user guide](https://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing MDS embedding\")\n",
    "clf = manifold.MDS(n_components=2, \n",
    "                   n_init=4, \n",
    "                   max_iter=200,\n",
    "                   n_jobs=-1,\n",
    "                   random_state=42,\n",
    "                   dissimilarity='euclidean')\n",
    "t0 = time()\n",
    "X_mds = clf.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"Done. Time: %f\" % (t1-t0))\n",
    "print(\"Stress: %f\" % clf.stress_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(X_mds,\n",
    "               \"MDS embedding of the digits (time %.2fs)\" %\n",
    "               (t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we can see that the separation of the clusters is similar to `PCA` and you can't tell which one is better, so let's continue and try one more technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distributed stochastic neighbor embedding (t-SNE)\n",
    "t-SNE is a very complex technique, which can often yield truly stunning results when reducing high-dimensional datasets. Here is a pretty good explanation of t-SNE from [Wikipedia](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding):   \n",
    "> \"The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimises the [Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the two distributions with respect to the locations of the points in the map. Note that whilst the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate.\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Understanding t-SNE in Simple Terms\n",
    "\n",
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a powerful technique for visualizing high-dimensional data in a lower-dimensional space. Here's how it works step by step:\n",
    "\n",
    "1. **Measure Similarity** ðŸ”  \n",
    "   - Compute how similar each data point is to the others in the original high-dimensional space.\n",
    "\n",
    "2. **Random Placement** ðŸŽ²  \n",
    "   - Map the data points into a lower-dimensional space (e.g., 2D or 3D) randomly.\n",
    "\n",
    "3. **Attraction & Repulsion** âš–ï¸  \n",
    "   - Similar points pull each other closer.  \n",
    "   - Dissimilar points push each other apart.\n",
    "\n",
    "4. **Iterative Refinement** ðŸ”„  \n",
    "   - Adjust each pointâ€™s position step by step until the lower-dimensional representation preserves the original clusters.\n",
    "\n",
    "ðŸ‘‰ The result? A visually meaningful representation where similar data points stay close, making patterns and clusters easier to interpret! ðŸŽ¨âœ¨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can watch this [video](https://www.youtube.com/embed/NEaUSP4YerM) for a more detailed explanation of how t-SNE works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ðŸ“ Note:**  \n",
    "> t-SNE is **great for visualization** but **not for feature extraction**!  \n",
    "> Unlike PCA or MDS, t-SNE does **not preserve distances** between pointsâ€”only their relative similarities.  \n",
    "> This means you shouldnâ€™t use t-SNE-transformed features for machine learning models. ðŸš«ðŸ¤–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = manifold.TSNE(n_components=2,\n",
    "                     perplexity=40,\n",
    "                     metric='euclidean',\n",
    "                     init='pca',\n",
    "                     verbose=1,\n",
    "                     random_state=42)\n",
    "t0 = time()\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"Computed t-SNE embedding in %fs.\" %(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(X_tsne,\n",
    "               \"t-SNE embedding of the digits (time %.2fs)\" %\n",
    "               (t1 - t0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, how cool is that?!   \n",
    "\n",
    "Remember, the algorithm made no use of the labels. The digit arrays were reduced into a two-dimensional space where similar digits ended up close together. We have six main clusters â€“ one for each digit class. But we also have a few smaller clusters â€“ notice the small group of twos that are somewhere between the 1s and the rest of the 2s? Or the 1s that are quite close to the 2s?   \n",
    "\n",
    "### Word of caution in t-SNE\n",
    "In this case, we have demonstrated the power of t-SNE as a tool for exploratory data analysis and to reveal natural groups, or clusters, within datasets.   \n",
    "\n",
    "However, t-SNE can also be very misleading at times and you are encouraged to investigate the effects of the various hyperparameters of the results when working with different datasets.   \n",
    "\n",
    "Check out [this article](https://distill.pub/2016/misread-tsne/) for some insights into t-SNE's various hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction on text\n",
    "\n",
    "We'll now work through the same dimensionality reduction techniques again, but this time using text data.\n",
    "\n",
    "### Loading and inspecting the data\n",
    "The data we'll be using are Twitter data from @CapeTownFreeway, posted during the year 2017. Let's import the NLP packages we'll need and take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/unsupervised_sprint/capetownfreeway.csv', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some preprocessing. A few of these columns will not be useful to us going forward, so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['source', 'in_reply_to_screen_name', 'is_retweet'],\n",
    "        axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function called **clean_tweet** to remove any URLs and any campaign-specific keywords that are frequently repeated in tweets. These tend to create a large amount of noise in the data, which will make the clustering process difficult later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    no_link_loc = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    no_num_loc = no_link_loc.lower()\n",
    "    for c in ['inbound','outbound','after','before','update','after','animals','roadworks',':',',',\n",
    "              '#','@','savewater', 'boozefreeroads','speedkillsfacts','saferoadsforall', 'sharetheroad',\n",
    "              'alwaysbuckleup', 'boozefreeroad','alwaysbuckleup','savekidslives','.','walksafe']:\n",
    "        no_num_loc = no_num_loc.replace(c, '')\n",
    "        \n",
    "    no_num_loc = no_num_loc.split(',')[0]\n",
    "    try:\n",
    "        return no_num_loc\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['text'].apply(clean_tweet)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the effect of cleaning our tweets, we quickly view a wordcloud illustration of the data. You may need to download the required `nltk` resource specified below. If so, uncomment the code and run it before you continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in df.clean_tweet:\n",
    "    words.extend(word_tokenize(i))\n",
    "    \n",
    "wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(frequencies = Counter(words))\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the ratio of 'delays' to 'cleared' is a clear eyesore for the traffic situation in Cape Town, with clear dominance from the N1 and N2 motorways into the city. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, we need to vectorize our text. Let's do that using  the term frequencyâ€“inverse document frequency technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequencyâ€“inverse document frequency (tfâ€“idf)\n",
    "**Term frequency (tf)** measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear far more often in long documents than shorter ones. Thus, the term frequency is often divided by the document length (a.k.a. the total number of terms in the document) as a way of normalisation: \n",
    "\n",
    "TF($t$) = (Number of times term $t$ appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "<i>(This is the relative frequency of each term in a document)</i>\n",
    "\n",
    "**Inverse document frequency (idf)** measures how important a term is. While computing tf, all terms are considered equally important. However, it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear many times but have little importance. Thus, we need to down-weight the frequent terms while up-weighting the rare ones, by computing the following: \n",
    "\n",
    "IDF($t$) = ln(Total number of documents / Number of documents containing term $t$).\n",
    "\n",
    "**tfâ€“idf** is the product of the values of **tf** and **idf**.\n",
    "\n",
    "\n",
    "**An example**\n",
    "> Consider a document containing 100 words wherein the word \"cat\" appears three times. The term frequency (i.e. tf) for \"cat\" is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word \"cat\" appears in one thousand of these. Then, the inverse document frequency (i.e. idf) is calculated as ln(10,000,000 / 1,000) = 4. Thus, the tfâ€“idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
    "\n",
    "More information can be found here: http://www.ultravioletanalytics.com/2016/11/18/tf-idf-basics-with-pandas-scikit-learn/.\n",
    "\n",
    "Luckily for us, there is a handy `sklearn` package called `TfidfVectorizer` that we can use to choose the number of features we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.8, max_features=50, stop_words='english')\n",
    "X = vectorizer.fit_transform(df.clean_tweet).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tfâ€“idf vectorizer creates a feature set of length 50, with the most frequently used words in the tweets we provided. It takes care of English stopwords such as \"in\", \"on\", \"the\", etc. which carry little to no meaning in terms of the content of the tweet and helps us to remove a large amount of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns=vocab)\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are now at a stage where we have tfâ€“idf vectors for each tweet, constructed from the 50 most popular words in the corpus. Let's now implement PCA, MDS, and t-SNE on these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
