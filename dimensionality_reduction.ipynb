{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced dimensionality reduction techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates advanced dimensionality reduction techniques such as Principal Component Analysis (PCA), Multi-Dimensional Scaling (MDS), and t-Distributed Stochastic Neighbor Embedding (t-SNE) on both image and text datasets. The project provides a hands-on exploration of how these techniques can be used to reduce high-dimensional data into lower-dimensional spaces while preserving meaningful patterns and structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "In this notebook we will:\n",
    "* Explain the following advanced dimensionality reduction techniques:\n",
    "    * Principal component analysis\n",
    "    * Multi-dimensional scaling\n",
    "    * t-SNE\n",
    "* Implement these techniques on an image dataset.\n",
    "* Implement these techniques on a text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction on images\n",
    "This notebook will provide a high-level overview of some of the more advanced techniques for dimensionality reduction. We will be using the [handwritten digits data set](http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits), which is a collection of images of handwritten digits between 0 and 9. The data was generated by a total of 43 people, who wrote a total of 5620 digits by hand which were then digitised and processed into 8x8 greyscale images.\n",
    "\n",
    "We'll start by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection, preprocessing)\n",
    "from matplotlib import offsetbox\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted',\n",
    "        rc={'figure.figsize': (15,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and inspecting the data\n",
    "The digits dataset is included as part of the `sklearn` library, which means loading it into the notebook is a breeze. For this train, we will simplify things by only looking at the first six digits in the dataset. We can use the `n_class` argument in the `load_digits()` function to select only the numbers from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits(n_class=6)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print(\"Total number of samples: \", n_samples)\n",
    "print(\"Features per sample: \", n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the image shows a selection from the 64-dimensional digits dataset. Each digit is represented as an 8x8 array of pixels, with values ranging between 0 (white) and 16 (black)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\n",
    "for idx, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)\n",
    "    ax.axis(\"off\")\n",
    "_ = fig.suptitle(\"A selection from the 64-dimensional digits dataset\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same concept applies to our dataset, only the arrays have been strung out into single rows of 64 values, ranging between 0 and 16. Let's take a look at the first digit in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you tell that the first array was a zero? Unless you are some sort of savant genius, you probably wouldn't be able to tell which digit is represented by looking only at the array values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction techniques\n",
    "Now what we are going to try and do is reduce the dimensionality of the entire dataset to just two dimensions (down from 64 dimensions â€“ one per pixel). We will use a few different dimensionality reduction techniques, and at each stage, we will plot the data and see how it is distributed in these two dimensions.   \n",
    "\n",
    "What is very important to note here is that none of these algorithms will be shown the labels of the data; they will be entirely unsupervised. In each case, we will plot the data in two dimensions, but include the known labels of the data points in the plots for our own validation.   \n",
    "\n",
    "For plotting, we are going to use the same `plot_embedding()` function defined in the original `sklearn` example code. This does a fantastic job of plotting the digits in a two-dimensional space, so there is no need to reinvent the wheel here.   \n",
    "\n",
    "**Note:** You will see the word **\"embedding\"** used in some of the comments and plots below. Simply put, an embedding is a representation of a vector in a different feature space. So in this case, the original digits exist as 64-dimensional arrays and are then reduced to just two dimensions. The resulting two-dimensional vectors are referred to as the embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and visualise the embedding vectors\n",
    "def plot_embedding(X, title=None):\n",
    "    \n",
    "    # normalise data\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  # just something big\n",
    "        for i in range(X.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-3:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_images = np.r_[shown_images, [X[i]]]\n",
    "            imagebox = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
    "                X[i])\n",
    "            ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA)\n",
    "The objective of PCA is to decompose a dataset into mutually orthogonal components that **each maximise the variance in the dataset**.   \n",
    "\n",
    "We will use PCA to decompose the dataset into the first two principal components, which will contain the largest and second-largest amounts of variance, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing PCA projection\")\n",
    "t0 = time()\n",
    "X_pca = decomposition.PCA(n_components=2).fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"Finished PCA projection in \" + str(t1-t0) + \"s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before using the awesome `plot_embedding()` function, let's plot the data, without labels, in the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1],\n",
    "                     sizes=(10, 200))\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you spot any obvious clusters? Perhaps one or two on the right side of the plot?   \n",
    "\n",
    "Now, let's plot the data with labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(X_pca,\n",
    "               \"Principal Components projection of the digits (time %.2fs)\" %\n",
    "               (t1 - t0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that digits do seem to group together, but there isn't the clearest separation between the different digits.   \n",
    "\n",
    "Now, let's take a look at some other techniques and see how they perform.   \n",
    "\n",
    "### Multi-dimensional scaling (MDS)\n",
    "The goal of MDS is to map features to a low-dimensional space **while preserving the distances** between observations in a given dataset.   \n",
    "\n",
    "MDS can be performed using algorithms that are either *metric* or *non-metric*. Non-metric approaches are typically used to preserve ordinality (order) within data. This is more of a necessity when there are categorical features present in the data. Since the data used here is entirely numeric, we will use a metric approach.   \n",
    "\n",
    "The **stress** is a measure of the degree to which distances between points in the original feature space correspond with the distances in the low-dimensional space. A lower stress value is preferred, and it is this quantity that is minimised by MDS.\n",
    "\n",
    "For more information on MDS, read the [`sklearn` user guide](https://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing MDS embedding\")\n",
    "clf = manifold.MDS(n_components=2, \n",
    "                   n_init=4, \n",
    "                   max_iter=200,\n",
    "                   n_jobs=-1,\n",
    "                   random_state=42,\n",
    "                   dissimilarity='euclidean')\n",
    "t0 = time()\n",
    "X_mds = clf.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"Done. Time: %f\" % (t1-t0))\n",
    "print(\"Stress: %f\" % clf.stress_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(X_mds,\n",
    "               \"MDS embedding of the digits (time %.2fs)\" %\n",
    "               (t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we can see that the separation of the clusters is similar to `PCA` and you can't tell which one is better, so let's continue and try one more technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distributed stochastic neighbor embedding (t-SNE)\n",
    "t-SNE is a very complex technique, which can often yield truly stunning results when reducing high-dimensional datasets. Here is a pretty good explanation of t-SNE from [Wikipedia](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding):   \n",
    "> \"The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimises the [Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the two distributions with respect to the locations of the points in the map. Note that whilst the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate.\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Understanding t-SNE in Simple Terms\n",
    "\n",
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a powerful technique for visualizing high-dimensional data in a lower-dimensional space. Here's how it works step by step:\n",
    "\n",
    "1. **Measure Similarity** ðŸ”  \n",
    "   - Compute how similar each data point is to the others in the original high-dimensional space.\n",
    "\n",
    "2. **Random Placement** ðŸŽ²  \n",
    "   - Map the data points into a lower-dimensional space (e.g., 2D or 3D) randomly.\n",
    "\n",
    "3. **Attraction & Repulsion** âš–ï¸  \n",
    "   - Similar points pull each other closer.  \n",
    "   - Dissimilar points push each other apart.\n",
    "\n",
    "4. **Iterative Refinement** ðŸ”„  \n",
    "   - Adjust each pointâ€™s position step by step until the lower-dimensional representation preserves the original clusters.\n",
    "\n",
    "ðŸ‘‰ The result? A visually meaningful representation where similar data points stay close, making patterns and clusters easier to interpret! ðŸŽ¨âœ¨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can watch this [video](https://www.youtube.com/embed/NEaUSP4YerM) for a more detailed explanation of how t-SNE works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ðŸ“ Note:**  \n",
    "> t-SNE is **great for visualization** but **not for feature extraction**!  \n",
    "> Unlike PCA or MDS, t-SNE does **not preserve distances** between pointsâ€”only their relative similarities.  \n",
    "> This means you shouldnâ€™t use t-SNE-transformed features for machine learning models. ðŸš«ðŸ¤–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = manifold.TSNE(n_components=2,\n",
    "                     perplexity=40,\n",
    "                     metric='euclidean',\n",
    "                     init='pca',\n",
    "                     verbose=1,\n",
    "                     random_state=42)\n",
    "t0 = time()\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"Computed t-SNE embedding in %fs.\" %(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(X_tsne,\n",
    "               \"t-SNE embedding of the digits (time %.2fs)\" %\n",
    "               (t1 - t0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, how cool is that?!   \n",
    "\n",
    "Remember, the algorithm made no use of the labels. The digit arrays were reduced into a two-dimensional space where similar digits ended up close together. We have six main clusters â€“ one for each digit class. But we also have a few smaller clusters â€“ notice the small group of twos that are somewhere between the 1s and the rest of the 2s? Or the 1s that are quite close to the 2s?   \n",
    "\n",
    "### Word of caution in t-SNE\n",
    "In this case, we have demonstrated the power of t-SNE as a tool for exploratory data analysis and to reveal natural groups, or clusters, within datasets.   \n",
    "\n",
    "However, t-SNE can also be very misleading at times and you are encouraged to investigate the effects of the various hyperparameters of the results when working with different datasets.   \n",
    "\n",
    "Check out [this article](https://distill.pub/2016/misread-tsne/) for some insights into t-SNE's various hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
